{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting you will need to have downloaded the Tiny-ImageNet-200 with the provided\n",
    "utility script and have run the `prepare_imagefolder_dataset.sh` bash script in order to\n",
    "prepare the data structure for the torch `ImageFolder` dataset that we will use later.\n",
    "\n",
    "You can use it like such `prepare_imagefolder_dataset.sh <path to val_annotations.txt> <path to images/>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the model\n",
    "\n",
    "For this part of the challenge I chose to work with a vision transformer pretrained with the DINOv2 framework.\n",
    "\n",
    "First of all we need to list availbale models and chose one. We can do that using the `torch.hub.list` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.hub.list('facebookresearch/dinov2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are plenty of pretrained pretrained models to chose from, the ones  that interests us are the models with a linear classifier head that have been trained on the ImageNet-1000 dataset, they are marked with a `_lc` suffix.\n",
    "\n",
    "Because of the limitations of the hardware I am working with I chose to use the base ViT size. \n",
    "\n",
    "I opted for the versions with extra registers because they peform slightly better than normal ViT with minimal computational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "Now that we have selected a model we can load it and prepare if for inference.\n",
    "\n",
    "The first step is loading it from the Torch Hub. We also set it in \"eval mode\" to deactivate DropOut layers.\n",
    "For now native torch quantization is only supported for CPU, so we keep the model on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg_lc')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "We will use the validation set from the tiny-imagenet-200 dataset for our inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet-1000_words.txt', \"r\") as f:\n",
    "    imagenet_1000_classes = list()\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        id = line.split('\\t')[0]\n",
    "        imagenet_1000_classes.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder('dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Vision Transformers embedding layer expects image widths and heights that are multiple of the the embedding patch size we have to resize the examples.\n",
    "\n",
    "I chose to resize the images to the closest integer multiple of 14, which is 70. Since tiny-imagenet-200 is a subset of imagenet we also apply the standard imagenet normalization to the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    T.Resize((70, 70))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a single example from the dataset. This seems to be the image of a fish/lobster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cls = dataset[999]\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a the id lookup table to see the corresponding human-readable class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_id2cls_map\n",
    "\n",
    "\n",
    "id2cls = build_id2cls_map(\"imagenet_words.txt\")\n",
    "\n",
    "id2cls[dataset.classes[cls]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform the image into an exmaple that can be processed by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = transforms(img)[None, ...] # adding an empty batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can forward it trough the model (with `torch.no_grad` context because we are doing inference, not training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check the models predictions.\n",
    "\n",
    "Because the model linear classifier has been trained on ImageNet-1000 we fist have to match the output to an ID. \n",
    "\n",
    "Because Tiny-ImageNet-200 is a subset of ImageNet-1000 we can't use the dataset `classes` attribute to determine that, so we have to use a file mapping the IDs of ImageNet-1000 to the class numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1 = torch.argmax(y_hat).item()\n",
    "id_predicted_top_1 = imagenet_1000_classes[top_1]\n",
    "cls_predicted_top_1 = id2cls[id_predicted_top_1]\n",
    "\n",
    "top_1, id_predicted_top_1, cls_predicted_top_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs seems a bit fishy, although we are not very far from the groud-truth, so let's check the top-5 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = torch.topk(y_hat, 5, dim=1)\n",
    "id_predicted_top_5 = [imagenet_1000_classes[t.item()] for t in top_5.indices[0]]\n",
    "cls_predicted_top_5 = [id2cls[id] for id in id_predicted_top_5]\n",
    "\n",
    "top_5, id_predicted_top_5, cls_predicted_top_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has predicted the right class in the top-5 outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization effects\n",
    "\n",
    "You can run the `benchmark.py` script to get benchmarks for the accuracy, inference time\n",
    "and model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "By quantizing a model we generally trade accuracy for space and compute time.\n",
    "\n",
    "We tested the normal model, and the model quantized with `toch.qint8` and \n",
    "`torch.float16` data types.\n",
    "\n",
    "The accuracy benchmark gives the following results :\n",
    "\n",
    "```\n",
    "\n",
    "===== Running ACCURACY benchmark ======\n",
    "\n",
    "Evaluating models accuracy using TOP-5\n",
    "Accuracy (normal):      0.7209                                                                                                                                                  \n",
    "Accuracy (quantized - qint8):   0.0718                                                                                                                                          \n",
    "Accuracy (quantized - float16): 0.7208\n",
    "```\n",
    "\n",
    "While the accuracy of the normal model and the `float16` quantized models are similar, \n",
    "the results with the `qint8` quantized model are catastrophic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time\n",
    "\n",
    "Quantized models are generally faster than the normal versions.\n",
    "\n",
    "In order to do a quick check of the inference time we can use the `%%timeit` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my machine I get a inference time of about 75 ms for the normal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the effect of quantization on the inference time of the model.\n",
    "\n",
    "First we use the `torch.quantization` module to convert our models `Linear` layers weigts to `torch.qint8` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n",
    "quantized_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run the cell with the magic function to get an estimate of the inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "with torch.no_grad():\n",
    "    quantized_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the quantized model I get an inference time of about 3/4th of the normal inference\n",
    "time.\n",
    "\n",
    "This result can seem disappointing, but we have to keep in mind that the default \n",
    "quantization backend can only convert `Linear` layers in our model. \n",
    "Operations such as self-attention, GELU etc.. are not quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark results are the following : \n",
    "\n",
    "```\n",
    "===== Running INFERENCE benchmark ======\n",
    "\n",
    "[-------------------------------------- ViT Forward --------------------------------------]                                                                                     \n",
    "                                    |  normal  |  quantized - qint8  |  quantized - float16\n",
    "6 threads: --------------------------------------------------------------------------------\n",
    "      torch.Size([1, 3, 518, 518])  |    3.1   |          2.1        |           2.7       \n",
    "      torch.Size([2, 3, 518, 518])  |    4.4   |          3.9        |           4.2       \n",
    "      torch.Size([4, 3, 518, 518])  |    8.8   |          8.2        |          10.8       \n",
    "      torch.Size([8, 3, 518, 518])  |   23.1   |         18.2        |          20.7       \n",
    "\n",
    "Times are in seconds (s).\n",
    "```\n",
    "\n",
    "The quantized models are slightly faster than the normal model, but the loss in accuracy \n",
    "for the `qint8` quantized model is not worth the small acceleration we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the `torch.autograd.profiler` to get more in-depth results about the \n",
    "inference timings of the normal and quantized models and find the bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running a more accurate benchmark using the `benchmark.py` script we get the following results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "\n",
    "with profiler.profile(with_stack=True, profile_memory=True) as p:\n",
    "    y_hat = model(x)\n",
    "\n",
    "print(p.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total'))\n",
    "\n",
    "with profiler.profile(with_stack=True, profile_memory=True) as p:\n",
    "    y_hat = quantized_model(x)\n",
    "\n",
    "print(p.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of the model\n",
    "\n",
    "One of the main purposes for quantization is running models on edge devices with limited\n",
    "memory and compute power let's check the scale of the size reduction for quantized\n",
    "models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the check the size that the models will occupy with the benchmark script :\n",
    "\n",
    "```\n",
    "===== Running SIZE benchmark ======\n",
    "\n",
    "Estimated size in memory\n",
    "Size (normal):   disk - 345.026460647583 MB  | memory - 361.698208 MB \n",
    "Size (quantized - qint8):        disk - 91.05587959289551 MB  | memory - 6.263808 MB \n",
    "Size (quantized - float16):      disk - 345.040864944458 MB  | memory - 6.263808 MB\n",
    "```\n",
    "\n",
    "The size reduction for the file of the `quint8` quantized model is sizeable, while the\n",
    "`float16` occupy almost the sme size on disk as the normal model even thoug the model is\n",
    "loaded with `float32` parameters.\n",
    "\n",
    "The size of the quantized models in memory is noticabley lower.\n",
    "\n",
    "These results are probably explained by some implementation quirks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
