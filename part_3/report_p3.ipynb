{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to ONNX and converting to TRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX\n",
    "\n",
    "We will use the same model as in part 1, a Vision Transformer pretrained with the DINOv2 \n",
    "framework.\n",
    "\n",
    "If we try to naively export the model to ONNX this will result in an error, because \n",
    "one of the operations used in the model is not yet supported by ONNX \n",
    "(upsample_bicubic2d_aa) as shown in this [PR](https://github.com/microsoft/onnxscript/pull/1208)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fix the function that is causing trouble first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from onnx_utils import fix_dinov2_for_onnx_export\n",
    "\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg_lc')\n",
    "model = fix_dinov2_for_onnx_export(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_utils import export_to_onnx\n",
    "\n",
    "onnx_program = export_to_onnx(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a quick check to verify that the output of the torch model and ONNX model are\n",
    "close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "\n",
    "x_torch = torch.randn(1, 3, 518, 518)\n",
    "y_hat_torch = model(x_torch)\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"model.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "x_ort = {ort_session.get_inputs()[0].name: to_numpy(x_torch)}\n",
    "y_hat_ort = ort_session.run(None, x_ort)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(y_hat_torch), y_hat_ort[0], rtol=1e-02, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a quick inference time benchmark on the Torch and ONNX models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.randn(1, 3, 518, 518)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "model(x_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"model.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "x_ort = {ort_session.get_inputs()[0].name: to_numpy(x_torch)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "y_hat_ort = ort_session.run(None, x_ort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ONNX model seems to be slightly faster than the Torch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ONNX models can be used with a GPU we can also run a little benchmark of the \n",
    "models on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "x_torch = x_torch.to(\"cuda\")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "model(x_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"model.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "x_ort = {ort_session.get_inputs()[0].name: to_numpy(x_torch)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "y_hat_ort = ort_session.run(None, x_ort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we notice a small improvement in timing when using the ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To TRT and beyond\n",
    "\n",
    "Now that we have a working ONNX program, it is time to try to export it to TensorRT.\n",
    "\n",
    "Of course we can't do it straight away (this would be too easy), because if we do\n",
    "we would get the following error : \n",
    "```\n",
    "[11/14/2024-10:20:05] [E] [TRT] ModelImporter.cpp:948: --- End node ---\n",
    "[11/14/2024-10:20:05] [E] [TRT] ModelImporter.cpp:951: ERROR: onnxOpCheckers.cpp:151 In function emptyOutputChecker:\n",
    "[8] This version of TensorRT doesn't support mode than 1 outputs for LayerNormalization nodes!\n",
    "[11/14/2024-10:20:05] [E] [TRT] ModelImporter.cpp:946: While parsing node number XXX [LayerNormalization -> \"getitem_xx\"]:\n",
    "[11/14/2024-10:20:05] [E] [TRT] ModelImporter.cpp:947: --- Begin node ---\n",
    "```\n",
    "\n",
    "Which basically tells us that our `LayerNormalization` layers output too much things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the ONNX graph with `netron` will give us a clue of what's happening.\n",
    "\n",
    "Our `LayerNormalization` output 3 things, the 2 supplementary outputs are the mean and \n",
    "deviation of the layer normalization, this is used to speed up training.\n",
    "This is probably an artifact fro the pretrained model. \n",
    "\n",
    "In the case of inference we do not use these values and can safely remove them.\n",
    "\n",
    "We can do that using the `onnx_graphsurgeon` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trt_utils import fix_graph_for_trt_export\n",
    "\n",
    "model = fix_graph_for_trt_export(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert it to a trt engine and serialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trt_utils import build_engine\n",
    "\n",
    "build_engine(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough the builder correctly parses the file, unfortunately the hardware I use is too \n",
    "old and I can't convert the model to trt with it, I get the following error :\n",
    "\n",
    "`IBuilder::buildSerializedNetwork: Error Code 9: API Usage Error (Target GPU SM 61 is not supported by this TensorRT release.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
